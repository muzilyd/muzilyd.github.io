---
title: Problems of Developing Parallel S of Complex Architecture Based on GPGPU Techno Hybrid Fluid-Based Model of Internet Traffic in Computer Networklogy and their Solutions
tags: 论文阅读
---

> 作业内容：
> 检索、阅读和理解一篇论文，然后
>
> - 概述其思想、理论、模型和算法；
> - 评述其创新点、特色和作用；
> - 指出其不足和可改进之处；
> - 为了理解该论文，可能需要阅读相关论文；
>
> 请列出（并下载附上）你阅读和参考的全部文献。
> 论文检索网站：
> 中山大学图书馆网站：<http://library.sysu.edu.cn/eresource/256>
> 检索入口：<https://ieeexplore.ieee.org>
>
> search:
> internet traffic model
> (filter applied: Conferences, Journals, Magazines, Early Access Articles)

本次高级计算机网络课程的作业，要求选取与网络流量模型相关的论文。因为以后主要围绕 GPU 做研究，因此在老师提供的论文列表中选取了这样一篇在考虑到 GPU 情况下的网络模型优化论文。这篇论文发表于 CICN15（在印度举办），团队来自“由俄罗斯第一任总统 B·N·叶利钦而得名的乌拉尔联邦大学”（没错，论文作者给出的校名真的有这么长；那中大是不是也可以叫“以中山先生…”），链接见 <https://ieeexplore.ieee.org/document/7546073>。

## 摘要

> 本文对开发基于并行混合流体的模型时遇到的问题进行思考，并提出了解决这些问题的方法。本文首先描述了在开发过程中出现的 GPU 性能下降的主要原因以及解决这些问题的方法。提供了一种利用路由邻接矩阵描述网络结构的方法。此外考虑了几种评估线矩阵求和的方法，并介绍了其中最有效的方法。

我将摘要粗略翻译了一下，得到了上述内容。不过我觉得就摘要的内容本身不是很连贯，在我对相关领域的工作比较陌生的情况下，看不出“首先”、“随后”、“此外”这三项工作有什么关联或递进关系，很难直接得到作者的研究思路。

## 论文主要思想与相关工作

> 2000 年，W. Misra 等人提出了一种基于流体的互联网流量模型（FBM, Fluid-Based Internet Traffic Model）。2012 年，M.K. Grebenkin 等人对 FBM 进行分析，认为应用于 FBM 的流量源模型是近似的，且没有考虑离散网络用户的行为特征。他们提出了一种名为“主干互联网通道中信息流的混合流体模型（HFBM, Hybrid Fluid-Based Model）”的改进模型，克服了以上缺点。HFBM 能够描述多服务网络中的流量，考虑到固有的 TCP 协议对源操作的反向网络工作负载影响，并考虑当前的特定用户策略来管理互联网访问速率。他们同时给出了 HFBM 在单路由器计算机网络中的单 CPU 实现。后续的研究者们将其并行化并移植到 GPU 上。这篇文章介绍了基于 GPGPU 技术在复杂体系结构的计算机网络中开发 Internet 流量并行 FBM 的经验和解决方法。

## 主要模型

论文通过归纳上文提到的几篇相关工作，得到了互联网流量模型中的几个关键指标及其公式：

- 时刻 $t$，图上的边 $l$，流 $i$ 的数据包发送率：$D_i^l(t)$
- 数据包到达率
- 流 $i$ 的数据窗口大小
- 时刻 $t$，边 $l$ 上的队列长度
- 流量源：按照用户的使用特征，可以将其大致分成“象”、“骡”、“鼠”三类，并各自使用不同的方式建模，分配不同的调度策略与接入速率

## 主要工作与算法

系统方向的很多研究，一般都围绕着“发现一个问题”->“得到解决方案”的思路进行。HFBM 并行化时出现的主要问题之一是，由网络结构引起的数据（输出/输入工作量、路由丢包率和往返时间）排列问题。算法的经典版本，使用邻接矩阵和关联矩阵。在 GPU 上使用时，因为所有的矩阵要么是针对分组行进路线计算的，要么是基于路由参数来计算的。在这种情况下，指定路线的数据需要建立对每个的全局存储器的连续访问机制的随机矩阵位置，在 GPU 上可能导致效率的显著下降。

为解决这一问题，论文的作者提出了一种改进的图 G（路由邻接矩阵）关联矩阵，用于代替提出了一种改进的图 G（路由邻接矩阵）关联矩阵。从存储器访问和 HFBM 计算算法实现的角度来看，按照路由邻接矩阵进行数据排列，能够足够准确地计算单独路由和单独路由器的参数。同时，他们建立了额外的数据结构，这些数据结构以更适合 GPU 的形式包含有关路由的信息。可以利用传统的矩阵和向量求和、转置和乘法算法，其效率接近最大。

然而，仍然缺少最直接有效的算法来计算矩阵的每一行的和，因此作者提出了三种方案：

- 顺序计算元素按照行数分布，每行后续位移在计算核心和并行列行求和（列并行）；
- 初步求和矩阵变换，进一步计算第一行和平行行求和（行并行）的元素分布；
- 通过多步金字塔排序算法（列-行并行）顺序分离每行总和。

在经过相同操作系统、相同图形加速器和相同数据集的同一台计算机进行的实验后，作者们得出第二种方案效果最优。

## 本文主要创新点、特色和作用

本文讨论了在 GPU 上并行 HFBM 主干网络流量实现过程中出现的主要问题及其解决方法。与基于 CPU 的混合流体模型计算算法相比，后者能够充分加速计算。

## 不足和可改进之处

这项工作本身是对他人工作成果的改进，并且本身属于较为“工程”向的研究，与创新点相比更多体现的是实际工作中的经验。

此外，这篇文章发表于 15 年，当时 CUDA 刚刚变得比较热门，比较多的研究是将传统算法移植到 GPU 上。但是，GPU 高带宽但延迟高的特性应用在流量控制领域，真的可以带来想象中的高收益吗？也许对于数据中心、云计算平台等，可以设计更加专用的硬件解决这些问题。

例如，近两年（2021 年），英伟达推出了 DPU，在网卡上直接集成了较强的算力，这样管理流量无需再经过宿主机，开销进一步减少了。且 DPU 的后续版本也将增加 GPU 核心，届时 GPU 将可以直接访问到流量，无需再与内存通信，在这种情况下可能需要重新设计优化新的并行流量控制算法。
