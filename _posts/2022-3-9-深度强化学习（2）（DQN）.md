---
redirect_from: /_posts/2022-3-9-深度强化学习（2）（DQN）.md
title: 2022-3-9-深度强化学习（2）（DQN）
tags: 
  - 深度强化学习
  - 算法
---

## DQN介绍
&emsp;&emsp;在使用Qlearning时，我们采用Q表进行更新，使用的状态都是离散的有限个状态集合S，但是面对状态很多的情况，维护的Q表很大，甚至很多时候，状态是连续的，那么就算离散化后，集合也很大，此时我们的传统方法，比如Q-Learning，根本无法在内存中维护这么大的一张Q表。所以我们采用价值函数的近似表示，使用神经网络的方法来进行近似。</br>
![DQNsimilar](https://github.com/muzilyd/blog-image/blob/202b4d1906b687260ffe0c3fa26e80ba82f26317/reinforcement%20learning/DQNsimilar.jpg)</br>
<br/>&emsp;&emsp;上面的图片展示了三种情况：对于状态价值函数，神经网络的输入是状态s的特征向量，输出是状态价值v^(s,w)。对于动作价值函数，有两种方法，一种是输入状态s的特征向量和动作a，输出对应的动作价值q^(s,a,w)，另一种是只输入状态s的特征向量，动作集合有多少个动作就有多少个输出q^(s,ai,w)。这里隐含了我们的动作是有限个的离散动作。</br>

## DQN基本流程</br>
&emsp;&emsp;本流程基于DQN NIPS 2015：</br>
&emsp;&emsp;算法输入：迭代轮数T，状态特征维度n, 动作集A, 步长α，衰减因子γ, 探索率ϵ, 当前Q网络Q，目标Q网络Q′, 批量梯度下降的样本数m,目标Q网络参数更新频率C。</br>
&emsp;&emsp;输出：Q网络参数</br>
&emsp;1. 随机初始化所有的状态和动作对应的价值Q.  随机初始化当前Q网络的所有参数w,初始化目标Q网络Q′的参数w′=w。清空经验回放的集合D。</br>
&emsp;2. for i from 1 to T，进行迭代。</br>
&emsp;&emsp;&emspa) 初始化S为当前状态序列的第一个状态, 拿到其特征向量ϕ(S)</br>
&emsp;&emsp;&emsp;b) 在Q网络中使用ϕ(S)作为输入，得到Q网络的所有动作对应的Q值输出。用ϵ−贪婪法在当前Q值输出中选择对应的动作A</br>
&emsp;&emsp;&emsp;c) 在状态S执行当前动作A,得到新状态S′对应的特征向量ϕ(S′)和奖励R$,是否终止状态is_end</br>
&emsp;&emsp;&emsp;d) 将{ϕ(S),A,R,ϕ(S′),is_end}这个五元组存入经验回放集合D</br>
&emsp;&emsp;&emsp;e) S=S′</br>
&emsp;&emsp;&emsp;f)  从经验回放集合D中采样m个样本{ϕ(Sj),Aj,Rj,ϕ(S′j),is_endj},j=1,2.,,,m，计算当前目标Q值yj：</br>
$$ f(x)=\left\{
\begin{aligned}
x & = & \cos(t) \\
y & = & \sin(t) \\
z & = & \frac xy
\end{aligned}
\right.
$$
&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;$$ yj=\left\{\begin{matrix} Rj，is_endj is true \\ Rj+γmaxa′Q′(ϕ(S′j),A′j,w′)，is_endj is false {matrix}\right. $$</br>
&emsp;&emsp;&emsp;g)  使用均方差损失函数1m∑j=1m(yj−Q(ϕ(Sj),Aj,w))2，通过神经网络的梯度反向传播来更新Q网络的所有参数w</br>
&emsp;&emsp;&emsp;h) 如果i%C=1,则更新目标Q网络参数w′=w</br>
&emsp;&emsp;&emsp;i) 如果S′是终止状态，当前轮迭代完毕，否则转到步骤b)</br>
&emsp;&emsp;注意，上述第二步的f步和g步的Q值计算也都需要通过Q网络计算得到。另外，实际应用中，为了算法较好的收敛，探索率ϵ需要随着迭代的进行而变小。</br>
