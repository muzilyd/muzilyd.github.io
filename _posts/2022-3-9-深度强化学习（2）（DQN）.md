---
redirect_from: /_posts/2022-3-9-深度强化学习（2）（DQN）.md
title: 2022-3-9-深度强化学习（2）（DQN）
tags: 
  - 深度强化学习
  - 算法
---

## DQN介绍
&emsp;&emsp;在使用Qlearning时，我们采用Q表进行更新，使用的状态都是离散的有限个状态集合S，但是面对状态很多的情况，维护的Q表很大，甚至很多时候，状态是连续的，那么就算离散化后，集合也很大，此时我们的传统方法，比如Q-Learning，根本无法在内存中维护这么大的一张Q表。所以我们采用价值函数的近似表示，使用神经网络的方法来进行近似。</br>
![DQNsimilar](https://github.com/muzilyd/blog-image/blob/202b4d1906b687260ffe0c3fa26e80ba82f26317/reinforcement%20learning/DQNsimilar.jpg)</br>
<br/>对于状态价值函数，神经网络的输入是状态s的特征向量，输出是状态价值v^(s,w)。对于动作价值函数，有两种方法，一种是输入状态s的特征向量和动作a，输出对应的动作价值q^(s,a,w)，另一种是只输入状态s的特征向量，动作集合有多少个动作就有多少个输出q^(s,ai,w)。这里隐含了我们的动作是有限个的离散动作。
