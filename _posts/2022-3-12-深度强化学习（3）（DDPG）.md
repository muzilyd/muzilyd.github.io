---
redirect_from: /_posts/2022-3-12-深度强化学习（3）（DDPG）.md
title: 2022-3-12-深度强化学习（3）（DDPG）
tags: 
  - 深度强化学习
  - 算法
---

## DDPG介绍
&emsp;&emsp;从DDPG这个名字看，它是由D（Deep）+D（Deterministic ）+ PG(Policy Gradient)组成。DDPG与Actor-Critic 形式差不多，需要有基于策略 Policy 的神经网络和基于价值 Value 的神经网络，但是为了体现 DQN 的思想，每种神经网络我们都需要再细分为两个，Policy Gradient 这边，我们有估计网络和现实网络，估计网络用来输出实时的动作,供 actor 在现实中实行,而现实网络则是用来更新价值网络系统的。所以我们再来看看价值系统这边，我们也有现实网络和估计网络，他们都在输出这个状态的价值，而输入端却有不同，状态现实网络这边会拿着从动作现实网络来的动作加上状态的观测值加以分析，而状态估计网络则是拿着当时 Actor 施加的动作当做输入。

## DDPG基本流程</br>
&emsp;&emsp;输入：Actor当前网络，Actor目标网络，Critic当前网络，Critic目标网络,参数分别为θ,θ′,w,w′,衰减因子γ,  软更新系数τ,批量梯度下降的样本数m,目标Q网络参数更新频率C。最大迭代次数T。随机噪音函数\mathcal{N}</br>
&emsp;&emsp;输出：最优Actor当前网络参数θ,Critic当前网络参数w</br>
&emsp;1. 随机初始化θ,w, w′=w,θ′=θ。清空经验回放的集合D</br>
&emsp;2. for i from 1 to T，进行迭代。</br>
&emsp;&emsp;&emsp;a) 初始化S为当前状态序列的第一个状态, 拿到其特征向量ϕ(S)</br>
&emsp;&emsp;&emsp;b) 在Actor当前网络基于状态S得到动作A=πθ(ϕ(S))+N</br>
&emsp;&emsp;&emsp;c) 执行动作A,得到新状态S′,奖励R,是否终止状态%is\_end$</br>
&emsp;&emsp;&emsp;d) 将{ϕ(S),A,R,ϕ(S′),is_end}这个五元组存入经验回放集合D</br>
&emsp;&emsp;&emsp;e) S=S'</br>
&emsp;&emsp;&emsp;f) 从经验回放集合D中采样m个样本{ϕ(Sj),Aj,Rj,ϕ(S′j),is_endj},j=1,2.,,,m，计算当前目标Q值yj：</br>
&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;$$ yj=\left\{\begin{matrix} Rj，is_endj is true \\\ Rj+γQ′(ϕ(S′j),πθ′(ϕ(S′j)),w′)，is_endj is false {matrix}\right. $$</br>
&emsp;&emsp;&emsp;g)  使用均方差损失函数1m∑j=1m(yj−Q(ϕ(Sj),Aj,w))2，(通过神经网络的梯度反向传播来更新Critic当前网络的所有参数w)</br>
&emsp;&emsp;&emsp;h)  使用J(θ)=−1m∑j=1mQ(si,ai,θ)，(通过神经网络的梯度反向传播来更新Actor当前网络的所有参数θ)</br>
&emsp;&emsp;&emsp;i) 如果T%C=1,则更新Critic目标网络和Actor目标网络参数：</br>
&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;w′←τw+(1−τ)w′</br>
&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;θ′←τθ+(1−τ)θ′</br>
&emsp;&emsp;&emsp;j) 如果S′是终止状态，当前轮迭代完毕，否则转到步骤b)</br>
&emsp;&emsp;以上就是DDPG算法的主流程，要注意的是上面2.f中的πθ′(ϕ(S′j))是通过Actor目标网络得到，而Q′(ϕ(S′j),πθ′(ϕ(S′j)),w′)则是通过Critic目标网络得到的。</br>
