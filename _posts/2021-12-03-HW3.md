---
title: HW3
tags:
  - 作业
---

> Assume a GPU architecture that contains 10 SIMD processors. Each SIMD instruction has a width of 32 and each SIMD processor contains 8 lanes for single-precision arithmetic and load/store instructions, meaning that each nondiverged SIMD instruction can produce 32 results every 4 cycles. Assume a kernel that has divergent branches that causes, on average, 80% of threads to be active. Assume that 70% of all SIMD instructions executed are single precision arithmetic and 20% are load/store. Because not all memory latencies are covered, assume an average SIMD instruction issue rate of 0.85. Assume that the GPU has a clock speed of 1.5 GHz.
>
> - Compute the throughput, in GFLOP/s, for this kernel on this GPU.

Your answers here.

> - Assume that you have the following choices, what is speedup in throughput for each of these improvements?
>   1. Increasing the number of single-precision lanes to 16.
>   2. Increasing the number of SIMD processors to 15 (assume this change doesn’t affect any other performance metrics and that the code scales to the additional processors) .
>   3. Adding a cache that will effectively reduce memory latency by 40%, which will increase instruction issue rate to 0.95.

Your answers here.

> In this exercise, we will examine several loops and analyze their potential for parallelization.
>
> - Does the following loop have a loop-carried dependency?
>
> ```c
> for (int i = 0; i < 100; ++i) {
>   A[i] = B[2 * i + 4];
>   B[4 * i + 5] = A[i];
> }
> ```

Your answers here.

> - In the following loop, find all the true dependences, output dependencies, and anti-dependencies. Eliminate the output dependencies and anti-dependencies by renaming.
>
> ```c
> for (int i = 0; i < 100; ++i) {
>   A[i] = A[i] * B[i]; /* S1 */
>   B[i] = A[i] + c;    /* S2 */
>   A[i] = C[i] * c;    /* S3 */
>   C[i] = D[i] * A[i]; /* S4 */
> }
> ```

Your answers here.

> - Consider the following loop, Are there dependences between S1, S2 and S3? Is this loop parallel? If not, show how to make it parallel.
>
> ```c
> for (int i = 0; i < 100; ++i) {
>   A[i] = B[i] + C[i];     /* S1 */
>   B[i + 1] = D[i] + E[i]; /* S2 */
>   c[i + 1] = D[i] * E[i]  /* S3 */
> }
> ```

Your answers here.

> List and describe at least four factors that influence the performance of GPU kernels. In other words, which runtime behaviors are caused by the kernel code cause a reduction in resource utilization during kernel execution?

Your answers here.

> Assume that we have a function for an application of the form , which gives the fraction of time that exactly processors are usable given that a total of processors are available. This means that
>
> $$
> \sum_{i=1}^pF\left(i,p\right)=1
> $$
>
> Assume that when processors are in use, the applications run times faster.
>
> - Rewrite Amdahl's Law so that it gives the speedup as a function of for some application.

Your answers here.

> - An application A runs on single processor for a time seconds. Different portions of its running time can improve if a larger number of processors is used. The figure below provides the details. How much speedup will A achieve when on 8 processors?
>
> | Fraction of T | 20% | 20% | 10% | 5%  | 15% | 20% | 10% |
> | :-----------: | :-: | :-: | :-: | :-: | :-: | :-: | :-: |
> | Processors(P) |  1  |  2  |  4  |  6  |  8  | 16  | 128 |

Your answers here.

> - Repeat for 32 processors and an infinite number of processors.

Your answers here.

> In this exercise, we examine the effect of the interconnection network topology on the CPI of programs running on a 64-processor distributed-memory multiprocessor. The processor clock rate is 2.0 GHz, and the base CPI of an application with all references hitting in the cache is 0.75. Assume that 0.2% of the instructions involve a remote communication reference. The cost of a remote communication reference is $\left(100+10\times h\right)$ ns, being the number of communication network hops that a remote reference has to make to the remote processor memory and back. Assume all communication links are bidirectional.
>
> - Calculate the worst-case remote communication cost when the 64 processors are arranged as a ring, as an $8\times 8$ processor grid, or as a hypercube(Hint: longest communication path on a $2^n$ hypercube has n links).

Your answers here.

> - Compare the base CPI of the application with no remote communication to the CPI achieved with each of the three topologies in last part.

Your answers here.
